{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing Notebook\n",
        "I will use this model to test a baseline deep reinforcement learning framework. More modularized code will be used in the consequent files in this folder. Please just use this notebook for reference. **All findings will be in the other notebooks and scripts.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import random\n",
        "from collections import deque\n",
        "import gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1o_EEumVnswul9MVsrdDwBch5rt7JTr0m\n",
            "To: c:\\Users\\Daniel\\Desktop\\UMich\\capstone\\ss24-capstone-team23-datallah-nkitts-steveso\\temporary_files\\merged.csv\n",
            "100%|██████████| 438k/438k [00:00<00:00, 6.62MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'../../temporary_files/merged.csv'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download the Data from Google Drive to the temporary folder\n",
        "merged_data_file_id = '1o_EEumVnswul9MVsrdDwBch5rt7JTr0m'\n",
        "merged_data_url = f'https://drive.google.com/uc?id={merged_data_file_id}'\n",
        "merged_data_filepath = '../../temporary_files/merged.csv'\n",
        "gdown.download(merged_data_url, merged_data_filepath, quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRXJC9W-EOFU"
      },
      "source": [
        "# Deep Q-Network\n",
        "In this notebook we will experiment with using a deep reinforcement learning model to forecast the Zillow Home Value Index (ZHVI). The temporal cutoff point will be December 31, 2021. Our test data will be the remaining ZHVI indices in 2022. The evaluation metric will be the mean standard error (MSE). Our baseline is a mean baseline model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h4nuglFGnD4"
      },
      "source": [
        "## Preprocess data\n",
        "We will be using our merged dataframe with various macroeconomic factors. A majority of the preprocessing has been done during data collection. The additional preprocessing for this model includes:\n",
        "- temporal train-test split,\n",
        "- quantitative column standardization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uRrwA2fOEIAV"
      },
      "outputs": [],
      "source": [
        "# import data\n",
        "merged = pd.read_csv(merged_data_filepath)\n",
        "### TEST Chicago FOR NOW ###\n",
        "city_nm = 'Chicago'\n",
        "merged = merged[merged.City == city_nm]\n",
        "### #################### ###\n",
        "merged.Date = pd.to_datetime(merged.Date)\n",
        "merged.sort_values('Date', inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PoR0C30fh9-T"
      },
      "outputs": [],
      "source": [
        "# create action space\n",
        "merged['pct_chng'] = merged.ZHVI.pct_change()\n",
        "# it isn't uncommon to see 7% swings in home value (0.583% month to month)\n",
        "# so will label anything within 3 - 7% as reasonable increase/decrease\n",
        "# anything less than that as relatively unchanged\n",
        "# anything more than that as significant increase/decrease\n",
        "def conditions(s):\n",
        "    if s > 0.07/12: return 2\n",
        "    elif s < -0.07/12: return -2\n",
        "    elif s >= 0.03/12 and s <= 0.07/12: return 1\n",
        "    elif s <= -0.03/12 and s >= -0.07/12: return -1\n",
        "    elif s > -0.03/12 and s < 0.03/12: return 0\n",
        "# apply conditions\n",
        "merged['change'] = merged.pct_chng.apply(conditions)\n",
        "# drop pct_chng and ZHVI so no data leakage\n",
        "merged.drop(['ZHVI', 'pct_chng'], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "g09FCY4YLf3A"
      },
      "outputs": [],
      "source": [
        "# grab numeric columns to scale\n",
        "numeric_cols = list(merged.drop(['City', 'Date', 'change'], axis = 1).columns)\n",
        "scaler = MinMaxScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sUqYLHL6DZGI"
      },
      "outputs": [],
      "source": [
        "# split based on year\n",
        "train = merged[merged.Date.dt.year < 2022]\n",
        "test = merged[merged.Date.dt.year >= 2022]\n",
        "# scale all data before creating X and y\n",
        "train_X = scaler.fit_transform(train[numeric_cols].astype(float))\n",
        "test_X = scaler.fit_transform(test[numeric_cols].astype(float))\n",
        "train_y = train.change.values\n",
        "test_y = test.change.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKSaPUu_UduT"
      },
      "source": [
        "## Construct Model\n",
        "I will be using PyTorch and creating my own model class. This will be a single layer neural net with a Q-learning agent. This will serve as my baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gU87Qe6tuWBt"
      },
      "outputs": [],
      "source": [
        "# create LSTM\n",
        "class QNetwork(torch.nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, hidden_dim, num_layers):\n",
        "    super().__init__()\n",
        "    self.lstm = nn.LSTM(input_size = input_dim, hidden_size = hidden_dim,\n",
        "                        num_layers = num_layers, batch_first = True)\n",
        "    self.linear = nn.Linear(in_features = hidden_dim, out_features = output_dim)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x, _ = self.lstm(state)\n",
        "    x = x[:, -1, :]\n",
        "    x = self.linear(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1NzcgDsyfbp9"
      },
      "outputs": [],
      "source": [
        "# create time series\n",
        "class TimeSeries:\n",
        "  def __init__(self, X, y, window_size):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.window_size = window_size\n",
        "    self.current_step = 0\n",
        "    self.data_len = len(self.X)\n",
        "\n",
        "  def reset(self):\n",
        "    self.current_step = self.window_size\n",
        "    return self.X[:self.current_step, :]\n",
        "\n",
        "  def step(self, action):\n",
        "    self.current_step += 1\n",
        "    done = self.current_step >= self.data_len - 1\n",
        "    next_state = self.X[self.current_step - self.window_size:self.current_step]\n",
        "    actual = self.y[self.current_step]\n",
        "    reward = -abs(actual - action)\n",
        "    return next_state, reward, done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rdEXg-LvUb96"
      },
      "outputs": [],
      "source": [
        "# create agent\n",
        "class DQNAgent:\n",
        "  def __init__(self, input_dim, output_dim, hidden_dim, window_size, lr, gamma, eps, \n",
        "               eps_decay, min_eps, memory_size, batch_size, num_layers = 1, seed = None,\n",
        "               QNetwork = QNetwork):\n",
        "    self.dqn = QNetwork(input_dim, output_dim, hidden_dim, num_layers)\n",
        "    self.dqn_target = QNetwork(input_dim, output_dim, hidden_dim, num_layers)\n",
        "    self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.window_size = window_size\n",
        "    self.loss_fn = nn.MSELoss()\n",
        "    self.optim = optim.Adam(self.dqn.parameters(), lr = lr)\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = eps\n",
        "    self.epsilon_decay = eps_decay\n",
        "    self.min_epsilon = min_eps\n",
        "    self.batch_size = batch_size\n",
        "    self.replay_memory_buffer = deque(maxlen = memory_size)\n",
        "    if seed is None:\n",
        "        self.rng = np.random.default_rng()\n",
        "    else:\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "  def select_action(self, state):\n",
        "    if self.rng.uniform() < self.epsilon:\n",
        "      action = self.rng.choice(self.output_dim)\n",
        "    else:\n",
        "      state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "      self.dqn.eval()\n",
        "      with torch.no_grad():\n",
        "          q_values = self.dqn(state)\n",
        "      self.dqn.train()\n",
        "      action = torch.argmax(q_values).item()\n",
        "    return action\n",
        "\n",
        "  def train(self, s0, a0, r, s1, done):\n",
        "    self.add_to_replay_memory(s0, a0, r, s1, done)\n",
        "\n",
        "    if done:\n",
        "      self.update_epsilon()\n",
        "      self.target_update()\n",
        "\n",
        "    if len(self.replay_memory_buffer) < self.batch_size:\n",
        "      return\n",
        "\n",
        "    mini_batch = self.get_random_sample_from_replay_mem()\n",
        "    state_batch = torch.from_numpy(np.stack([i[0] for i in mini_batch])).float()\n",
        "    action_batch = torch.from_numpy(np.vstack([i[1] for i in mini_batch])).int() #reshape(1, self.batch_size, 1)\n",
        "    reward_batch = torch.from_numpy(np.vstack([i[2] for i in mini_batch])).float()\n",
        "    next_state_batch = torch.from_numpy(np.stack([i[3] for i in mini_batch])).float()\n",
        "    done_list = torch.from_numpy(np.vstack([i[4] for i in mini_batch]).astype(np.uint8)).float()\n",
        "    \n",
        "    current_qs = self.dqn(state_batch)\n",
        "    current_q  = current_qs.gather(1, action_batch.type(torch.int64))\n",
        "    next_q, _  = self.dqn_target(next_state_batch).max(dim = 1)\n",
        "    next_q     = next_q.view(self.batch_size, 1)\n",
        "    Q_targets  = reward_batch + self.gamma * next_q * (1 - done_list)\n",
        "    loss       = self.loss_fn(current_q, Q_targets.detach())\n",
        "    self.optim.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optim.step()\n",
        "\n",
        "  def add_to_replay_memory(self, state, action, reward, next_state, done):\n",
        "    self.replay_memory_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def get_random_sample_from_replay_mem(self):\n",
        "    random_sample = random.sample(self.replay_memory_buffer, self.batch_size)\n",
        "    return random_sample\n",
        "\n",
        "  def update_epsilon(self):\n",
        "    if self.epsilon > self.min_epsilon:\n",
        "      self.epsilon *= self.epsilon_decay\n",
        "      self.epsilon = max(self.min_epsilon, self.epsilon)\n",
        "\n",
        "  def target_update(self):\n",
        "    self.dqn_target.load_state_dict(self.dqn.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define training looper\n",
        "def episode_loop(X, y, max_reward = 0, maxlen = 100, window_size = 7, seed = 0, num_layers = 1,\n",
        "                 hidden_dim = 24, lr = 0.001, gamma = 0.99, eps = 1, eps_decay = 0.995, \n",
        "                 min_eps = 0.01, memory_size = 36, batch_size = 12, num_episodes = 600):\n",
        "  reward_queue = deque(maxlen = maxlen)\n",
        "  all_rewards = []\n",
        "  all_rewards_each_step = []\n",
        "  env = TimeSeries(X, y, window_size)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  input_dim = X.shape[1]\n",
        "  output_dim = len(np.unique(y[~np.isnan(y)]))\n",
        "  agent = DQNAgent(input_dim, output_dim, hidden_dim, window_size, lr, gamma, eps, \n",
        "                   eps_decay, min_eps, memory_size, batch_size, num_layers, seed)\n",
        "  # iterate through episodes and train\n",
        "  for i in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    episodic_reward = 0\n",
        "    episode_rewards = []\n",
        "    while not done:\n",
        "      action = agent.select_action(np.squeeze(state))\n",
        "      next_state, reward, done = env.step(action)\n",
        "      episode_rewards.append(reward)\n",
        "      episodic_reward += reward\n",
        "      agent.train(state, action, reward, next_state, done)\n",
        "      state = next_state\n",
        "    all_rewards.append(episodic_reward)\n",
        "    all_rewards_each_step.append(episode_rewards)\n",
        "    reward_queue.append(episodic_reward)\n",
        "    if (i + 1) % 10 == 0 and len(reward_queue) == 100 and (i + 1) % 10 == 0:\n",
        "      print(f'Training episode {i + 1}, reward: {episodic_reward}', end='')\n",
        "    elif (i + 1) % 10 == 0: \n",
        "      print(f'Training episode {i + 1}, reward: {episodic_reward}')\n",
        "    if len(reward_queue) == 100:\n",
        "      avg_reward = sum(reward_queue) / 100\n",
        "      if (i + 1) % 10 == 0:\n",
        "          print(f', moving average reward: {avg_reward}')\n",
        "  print('Average reward over 100 episodes: ', max_reward)\n",
        "  # return variables for viz\n",
        "  return all_rewards, all_rewards_each_step, agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training episode 10, reward: -224.0\n",
            "Training episode 20, reward: -239.0\n",
            "Training episode 30, reward: -229.0\n",
            "Training episode 40, reward: -212.0\n",
            "Training episode 50, reward: -197.0\n",
            "Training episode 60, reward: -208.0\n",
            "Training episode 70, reward: -184.0\n",
            "Training episode 80, reward: -171.0\n",
            "Training episode 90, reward: -186.0\n",
            "Training episode 100, reward: -196.0, moving average reward: -212.62\n",
            "Training episode 110, reward: -179.0, moving average reward: -205.05\n",
            "Training episode 120, reward: -155.0, moving average reward: -197.47\n",
            "Training episode 130, reward: -142.0, moving average reward: -189.89\n",
            "Training episode 140, reward: -135.0, moving average reward: -182.33\n",
            "Training episode 150, reward: -131.0, moving average reward: -174.69\n",
            "Training episode 160, reward: -131.0, moving average reward: -167.89\n",
            "Training episode 170, reward: -143.0, moving average reward: -161.91\n",
            "Training episode 180, reward: -136.0, moving average reward: -155.89\n",
            "Training episode 190, reward: -121.0, moving average reward: -150.35\n",
            "Training episode 200, reward: -106.0, moving average reward: -144.61\n",
            "Training episode 210, reward: -116.0, moving average reward: -139.04\n",
            "Training episode 220, reward: -109.0, moving average reward: -133.35\n",
            "Training episode 230, reward: -96.0, moving average reward: -128.68\n",
            "Training episode 240, reward: -94.0, moving average reward: -124.56\n",
            "Training episode 250, reward: -106.0, moving average reward: -120.58\n",
            "Training episode 260, reward: -90.0, moving average reward: -115.97\n",
            "Training episode 270, reward: -115.0, moving average reward: -112.3\n",
            "Training episode 280, reward: -84.0, moving average reward: -108.46\n",
            "Training episode 290, reward: -93.0, moving average reward: -104.83\n",
            "Training episode 300, reward: -92.0, moving average reward: -101.43\n",
            "Training episode 310, reward: -88.0, moving average reward: -99.1\n",
            "Training episode 320, reward: -79.0, moving average reward: -96.2\n",
            "Training episode 330, reward: -84.0, moving average reward: -92.71\n",
            "Training episode 340, reward: -75.0, moving average reward: -89.35\n",
            "Training episode 350, reward: -78.0, moving average reward: -87.12\n",
            "Training episode 360, reward: -75.0, moving average reward: -84.28\n",
            "Training episode 370, reward: -74.0, moving average reward: -82.64\n",
            "Training episode 380, reward: -75.0, moving average reward: -80.19\n",
            "Training episode 390, reward: -63.0, moving average reward: -77.93\n",
            "Training episode 400, reward: -67.0, moving average reward: -75.48\n",
            "Training episode 410, reward: -66.0, moving average reward: -73.46\n",
            "Training episode 420, reward: -54.0, moving average reward: -71.91\n",
            "Training episode 430, reward: -76.0, moving average reward: -70.49\n",
            "Training episode 440, reward: -62.0, moving average reward: -68.79\n",
            "Training episode 450, reward: -65.0, moving average reward: -67.19\n",
            "Training episode 460, reward: -56.0, moving average reward: -66.07\n",
            "Training episode 470, reward: -48.0, moving average reward: -63.61\n",
            "Training episode 480, reward: -45.0, moving average reward: -62.41\n",
            "Training episode 490, reward: -63.0, moving average reward: -61.34\n",
            "Training episode 500, reward: -49.0, moving average reward: -60.45\n",
            "Training episode 510, reward: -64.0, moving average reward: -58.67\n",
            "Training episode 520, reward: -46.0, moving average reward: -57.72\n",
            "Training episode 530, reward: -57.0, moving average reward: -57.01\n",
            "Training episode 540, reward: -57.0, moving average reward: -56.36\n",
            "Training episode 550, reward: -51.0, moving average reward: -55.67\n",
            "Training episode 560, reward: -44.0, moving average reward: -54.56\n",
            "Training episode 570, reward: -43.0, moving average reward: -53.88\n",
            "Training episode 580, reward: -44.0, moving average reward: -53.11\n",
            "Training episode 590, reward: -49.0, moving average reward: -52.48\n",
            "Training episode 600, reward: -51.0, moving average reward: -51.94\n",
            "Average reward over 100 episodes:  0\n"
          ]
        }
      ],
      "source": [
        "all_rewards, all_rewards_each_step, agent = episode_loop(train_X, train_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Model & Rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QNetwork(\n",
              "  (lstm): LSTM(82, 24, batch_first=True)\n",
              "  (linear): Linear(in_features=24, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.dqn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "name = 'base_Chicago_single_layer_test'\n",
        "torch.save(agent.dqn, f'models/{name}.pth')\n",
        "np.save(f'rewards/averaged/{name}.npy', np.array(all_rewards))\n",
        "np.save(f'rewards/episodic/{name}.npy', np.array(all_rewards_each_step))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test\n",
        "Use the held out data to test model performance and assess total reward. The closer the reward is to 0, the better. Given the architecture, the reward cannot be positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QNetwork(\n",
              "  (lstm): LSTM(82, 24, batch_first=True)\n",
              "  (linear): Linear(in_features=24, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reload model\n",
        "loaded_model = torch.load('models/base_Chicago_single_layer.pth')\n",
        "loaded_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create select_action function for testing\n",
        "def select_action_test(model, state):\n",
        "    state = torch.from_numpy(np.squeeze(state)).float().unsqueeze(0)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        q_values = model(state)\n",
        "    action = torch.argmax(q_values).item()\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute Reward on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-1.0\n",
            "-1.0\n",
            "-3.0\n",
            "-2.0\n",
            "Total reward on new data: -7.0\n"
          ]
        }
      ],
      "source": [
        "# init test env\n",
        "# test for a year in the future\n",
        "env_test = TimeSeries(test_X, test_y, window_size = 7)\n",
        "state = env_test.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "# compute total reward\n",
        "while not done:\n",
        "    action = select_action_test(loaded_model, state)\n",
        "    next_state, reward, done = env_test.step(action)\n",
        "    total_reward += reward\n",
        "    state = next_state\n",
        "    print(reward)\n",
        "print(f\"Total reward on new data: {total_reward}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
