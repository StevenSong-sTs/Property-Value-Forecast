{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing Notebook\n",
        "I will use this model to test a baseline deep reinforcement learning framework. More modularized code will be used in the consequent files in this folder. Please just use this notebook for reference. **All findings will be in the other notebooks and scripts.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import random\n",
        "from collections import deque\n",
        "import gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1o_EEumVnswul9MVsrdDwBch5rt7JTr0m\n",
            "To: c:\\Users\\Daniel\\Desktop\\UMich\\capstone\\ss24-capstone-team23-datallah-nkitts-steveso\\temporary_files\\merged.csv\n",
            "100%|██████████| 438k/438k [00:00<00:00, 9.00MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'../../temporary_files/merged.csv'"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download the Data from Google Drive to the temporary folder\n",
        "merged_data_file_id = '1o_EEumVnswul9MVsrdDwBch5rt7JTr0m'\n",
        "merged_data_url = f'https://drive.google.com/uc?id={merged_data_file_id}'\n",
        "merged_data_filepath = '../../temporary_files/merged.csv'\n",
        "gdown.download(merged_data_url, merged_data_filepath, quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRXJC9W-EOFU"
      },
      "source": [
        "# Deep Q-Network\n",
        "In this notebook we will experiment with using a deep reinforcement learning model to forecast the Zillow Home Value Index (ZHVI). The temporal cutoff point will be December 31, 2022. Our test data will be the remaining ZHVI indices in 2023 and 2024. The evaluation metric will be the mean standard error (MSE). Our baseline is a mean baseline model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h4nuglFGnD4"
      },
      "source": [
        "## Preprocess data\n",
        "We will be using our merged dataframe with various macroeconomic factors. A majority of the preprocessing has been done during data collection. The additional preprocessing for this model includes:\n",
        "- temporal train-test split,\n",
        "- quantitative column standardization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "uRrwA2fOEIAV"
      },
      "outputs": [],
      "source": [
        "# import data\n",
        "merged = pd.read_csv(merged_data_filepath)\n",
        "### TEST Chicago FOR NOW ###\n",
        "city_nm = 'Chicago'\n",
        "merged = merged[merged.City == city_nm]\n",
        "### #################### ###\n",
        "merged.Date = pd.to_datetime(merged.Date)\n",
        "merged.sort_values('Date', inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "PoR0C30fh9-T"
      },
      "outputs": [],
      "source": [
        "# create action space\n",
        "merged['pct_chng'] = merged.ZHVI.pct_change()\n",
        "# it isn't uncommon to see 7% swings in home value (0.583% month to month)\n",
        "# so will label anything within 3 - 7% as reasonable increase/decrease\n",
        "# anything less than that as relatively unchanged\n",
        "# anything more than that as significant increase/decrease\n",
        "def conditions(s):\n",
        "    if s > 0.07/12: return 2\n",
        "    elif s < -0.07/12: return -2\n",
        "    elif s >= 0.03/12 and s <= 0.07/12: return 1\n",
        "    elif s <= -0.03/12 and s >= -0.07/12: return -1\n",
        "    elif s > -0.03/12 and s < 0.03/12: return 0\n",
        "# apply conditions\n",
        "merged['change'] = merged.pct_chng.apply(conditions)\n",
        "# drop pct_chng and ZHVI so no data leakage\n",
        "merged.drop(['ZHVI', 'pct_chng'], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "g09FCY4YLf3A"
      },
      "outputs": [],
      "source": [
        "# grab numeric columns to scale\n",
        "numeric_cols = list(merged.drop(['City', 'Date', 'change'], axis = 1).columns)\n",
        "scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "sUqYLHL6DZGI"
      },
      "outputs": [],
      "source": [
        "# split based on year\n",
        "train = merged[merged.Date.dt.year < 2022]\n",
        "test = merged[merged.Date.dt.year >= 2022]\n",
        "# scale all data before creating X and y\n",
        "train_X = scaler.fit_transform(train[numeric_cols].astype(float))\n",
        "test_X = scaler.fit_transform(test[numeric_cols].astype(float))\n",
        "train_y = train.change.values\n",
        "test_y = test.change.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKSaPUu_UduT"
      },
      "source": [
        "## Construct Model\n",
        "I will be using PyTorch and creating my own model class. This will be a single layer neural net with a Q-learning agent. This will serve as my baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "gU87Qe6tuWBt"
      },
      "outputs": [],
      "source": [
        "# create LSTM\n",
        "class QNetwork(torch.nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, hidden_dim, num_layers):\n",
        "    super().__init__()\n",
        "    self.lstm = nn.LSTM(input_size = input_dim, hidden_size = hidden_dim,\n",
        "                        num_layers = num_layers, batch_first = True)\n",
        "    self.linear = nn.Linear(in_features = hidden_dim, out_features = output_dim)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x, _ = self.lstm(state)\n",
        "    x = x[:, -1, :]\n",
        "    x = self.linear(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "1NzcgDsyfbp9"
      },
      "outputs": [],
      "source": [
        "# create time series\n",
        "class TimeSeries:\n",
        "  def __init__(self, X, y, window_size):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.window_size = window_size\n",
        "    self.current_step = 0\n",
        "    self.data_len = len(self.X)\n",
        "\n",
        "  def reset(self):\n",
        "    self.current_step = self.window_size\n",
        "    return self.X[:self.current_step, :]\n",
        "\n",
        "  def step(self, action):\n",
        "    self.current_step += 1\n",
        "    done = self.current_step >= self.data_len - 1\n",
        "    next_state = self.X[self.current_step - self.window_size:self.current_step]\n",
        "    actual = self.y[self.current_step]\n",
        "    reward = -abs(actual - action)\n",
        "    return next_state, reward, done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "rdEXg-LvUb96"
      },
      "outputs": [],
      "source": [
        "# create agent\n",
        "class DQNAgent:\n",
        "  def __init__(self, input_dim, output_dim, hidden_dim, window_size, lr, gamma, eps, \n",
        "               eps_decay, min_eps, memory_size, batch_size, num_layers = 1, seed = None):\n",
        "    self.dqn = QNetwork(input_dim, output_dim, hidden_dim, num_layers)\n",
        "    self.dqn_target = QNetwork(input_dim, output_dim, hidden_dim, num_layers)\n",
        "    self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.window_size = window_size\n",
        "    self.loss_fn = nn.MSELoss()\n",
        "    self.optim = optim.Adam(self.dqn.parameters(), lr = lr)\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = eps\n",
        "    self.epsilon_decay = eps_decay\n",
        "    self.min_epsilon = min_eps\n",
        "    self.batch_size = batch_size\n",
        "    self.replay_memory_buffer = deque(maxlen = memory_size)\n",
        "    if seed is None:\n",
        "        self.rng = np.random.default_rng()\n",
        "    else:\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "  def select_action(self, state):\n",
        "    if self.rng.uniform() < self.epsilon:\n",
        "      action = self.rng.choice(self.output_dim)\n",
        "    else:\n",
        "      state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "      self.dqn.eval()\n",
        "      with torch.no_grad():\n",
        "          q_values = self.dqn(state)\n",
        "      self.dqn.train()\n",
        "      action = torch.argmax(q_values).item()\n",
        "    return action\n",
        "\n",
        "  def train(self, s0, a0, r, s1, done):\n",
        "    self.add_to_replay_memory(s0, a0, r, s1, done)\n",
        "\n",
        "    if done:\n",
        "      self.update_epsilon()\n",
        "      self.target_update()\n",
        "\n",
        "    if len(self.replay_memory_buffer) < self.batch_size:\n",
        "      return\n",
        "\n",
        "    mini_batch = self.get_random_sample_from_replay_mem()\n",
        "    state_batch = torch.from_numpy(np.stack([i[0] for i in mini_batch])).float()\n",
        "    action_batch = torch.from_numpy(np.vstack([i[1] for i in mini_batch])).int() #reshape(1, self.batch_size, 1)\n",
        "    reward_batch = torch.from_numpy(np.vstack([i[2] for i in mini_batch])).float()\n",
        "    next_state_batch = torch.from_numpy(np.stack([i[3] for i in mini_batch])).float()\n",
        "    done_list = torch.from_numpy(np.vstack([i[4] for i in mini_batch]).astype(np.uint8)).float()\n",
        "    \n",
        "    current_qs = self.dqn(state_batch)\n",
        "    current_q  = current_qs.gather(1, action_batch.type(torch.int64))\n",
        "    next_q, _  = self.dqn_target(next_state_batch).max(dim = 1)\n",
        "    next_q     = next_q.view(self.batch_size, 1)\n",
        "    Q_targets  = reward_batch + self.gamma * next_q * (1 - done_list)\n",
        "    loss       = self.loss_fn(current_q, Q_targets.detach())\n",
        "    self.optim.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optim.step()\n",
        "\n",
        "  def add_to_replay_memory(self, state, action, reward, next_state, done):\n",
        "    self.replay_memory_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def get_random_sample_from_replay_mem(self):\n",
        "    random_sample = random.sample(self.replay_memory_buffer, self.batch_size)\n",
        "    return random_sample\n",
        "\n",
        "  def update_epsilon(self):\n",
        "    if self.epsilon > self.min_epsilon:\n",
        "      self.epsilon *= self.epsilon_decay\n",
        "      self.epsilon = max(self.min_epsilon, self.epsilon)\n",
        "\n",
        "  def target_update(self):\n",
        "    self.dqn_target.load_state_dict(self.dqn.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDdbBTKiiloL",
        "outputId": "6f27c4ca-21b3-41ff-f23e-8b3208434ccc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training episode 10, reward: -275.0\n",
            "Training episode 20, reward: -279.0\n",
            "Training episode 30, reward: -246.0\n",
            "Training episode 40, reward: -242.0\n",
            "Training episode 50, reward: -246.0\n",
            "Training episode 60, reward: -217.0\n",
            "Training episode 70, reward: -221.0\n",
            "Training episode 80, reward: -205.0\n",
            "Training episode 90, reward: -193.0\n",
            "Training episode 100, reward: -186.0\n",
            ", moving average reward: -228.34\n",
            "Training episode 110, reward: -173.0\n",
            ", moving average reward: -219.77\n",
            "Training episode 120, reward: -158.0\n",
            ", moving average reward: -212.33\n",
            "Training episode 130, reward: -176.0\n",
            ", moving average reward: -204.89\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[199], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m   next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     23\u001b[0m   episodic_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m---> 24\u001b[0m   \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m   state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     26\u001b[0m reward_queue\u001b[38;5;241m.\u001b[39mappend(episodic_reward)\n",
            "Cell \u001b[1;32mIn[198], line 62\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[1;34m(self, s0, a0, r, s1, done)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     61\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Daniel\\Desktop\\UMich\\capstone\\ss24-capstone-team23-datallah-nkitts-steveso\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:374\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprofile_hook_step\u001b[39m(func: Callable[_P, R]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Callable[_P, R]:\n\u001b[1;32m--> 374\u001b[0m     \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m R:\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m    377\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# # train\n",
        "# max_reward = 0\n",
        "# reward_queue = deque(maxlen = 100)\n",
        "# X, y, window_size = train_X, train_y, 7\n",
        "# env = TimeSeries(X, y, window_size)\n",
        "# seed = 0\n",
        "# random.seed(seed)\n",
        "# np.random.seed(seed)\n",
        "# torch.manual_seed(seed)\n",
        "# input_dim = X.shape[1]\n",
        "# agent = DQNAgent(input_dim, output_dim = 5, hidden_dim = 24, window_size = window_size,\n",
        "#                  lr = 0.001, gamma = 0.99, eps = 1, eps_decay = 0.995, min_eps = 0.01,\n",
        "#                  memory_size = 36, batch_size = 12, seed = None)\n",
        "\n",
        "# num_episodes = 1000\n",
        "# for i in range(num_episodes):\n",
        "#   state = env.reset()\n",
        "#   done = False\n",
        "#   episodic_reward = 0\n",
        "#   while not done:\n",
        "#     action = agent.select_action(np.squeeze(state))\n",
        "#     next_state, reward, done = env.step(action)\n",
        "#     episodic_reward += reward\n",
        "#     agent.train(state, action, reward, next_state, done)\n",
        "#     state = next_state\n",
        "#   reward_queue.append(episodic_reward)\n",
        "#   if (i + 1) % 10 == 0:\n",
        "#     print(f'Training episode {i + 1}, reward: {episodic_reward}\\n', end='')\n",
        "#   if len(reward_queue) == 100:\n",
        "#     avg_reward = sum(reward_queue) / 100\n",
        "#     if (i + 1) % 10 == 0:\n",
        "#         print(f', moving average reward: {avg_reward}')\n",
        "#     if avg_reward > max_reward:\n",
        "#       max_reward = avg_reward\n",
        "\n",
        "# print('Average reward over 100 episodes: ', max_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define training looper\n",
        "def episode_loop(X, y, max_reward = 0, maxlen = 100, window_size = 7, seed = 0, num_layers = 1,\n",
        "                 hidden_dim = 24, lr = 0.001, gamma = 0.99, eps = 1, eps_decay = 0.995, \n",
        "                 min_eps = 0.01, memory_size = 36, batch_size = 12, num_episodes = 1000):\n",
        "  reward_queue = deque(maxlen = maxlen)\n",
        "  all_rewards = []\n",
        "  all_rewards_each_step = []\n",
        "  env = TimeSeries(X, y, window_size)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  input_dim = X.shape[1]\n",
        "  output_dim = len(np.unique(y[~np.isnan(y)]))\n",
        "  agent = DQNAgent(input_dim, output_dim, hidden_dim, window_size, lr, gamma, eps, \n",
        "                   eps_decay, min_eps, memory_size, batch_size, num_layers, seed)\n",
        "  # iterate through episodes and train\n",
        "  for i in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    episodic_reward = 0\n",
        "    episode_rewards = []\n",
        "    while not done:\n",
        "      action = agent.select_action(np.squeeze(state))\n",
        "      next_state, reward, done = env.step(action)\n",
        "      episode_rewards.append(reward)\n",
        "      episodic_reward += reward\n",
        "      agent.train(state, action, reward, next_state, done)\n",
        "      state = next_state\n",
        "    all_rewards.append(episodic_reward)\n",
        "    all_rewards_each_step.append(episode_rewards)\n",
        "    reward_queue.append(episodic_reward)\n",
        "    if (i + 1) % 10 == 0 and len(reward_queue) == 100 and (i + 1) % 10 == 0:\n",
        "      print(f'Training episode {i + 1}, reward: {episodic_reward}', end='')\n",
        "    elif (i + 1) % 10 == 0: \n",
        "      print(f'Training episode {i + 1}, reward: {episodic_reward}')\n",
        "    if len(reward_queue) == 100:\n",
        "      avg_reward = sum(reward_queue) / 100\n",
        "      if (i + 1) % 10 == 0:\n",
        "          print(f', moving average reward: {avg_reward}')\n",
        "      if avg_reward > max_reward:\n",
        "        max_reward = avg_reward\n",
        "  print('Average reward over 100 episodes: ', max_reward)\n",
        "  # return variables for viz\n",
        "  return all_rewards, all_rewards_each_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Model & Rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('lstm.weight_ih_l0',\n",
              "              tensor([[ 0.0474,  0.1087, -0.1597,  ...,  0.0491,  0.0036,  0.3580],\n",
              "                      [-0.0468, -0.2009, -0.0565,  ..., -0.3093, -0.4821, -0.4452],\n",
              "                      [ 0.7847,  0.3350,  0.0570,  ...,  0.1481,  0.2430, -0.0501],\n",
              "                      ...,\n",
              "                      [-0.0957, -0.0662, -0.0381,  ..., -0.0585, -0.0139, -0.0667],\n",
              "                      [-0.4215, -0.4411,  0.4478,  ...,  0.2081,  0.2418,  0.1124],\n",
              "                      [-0.1751, -0.1160, -0.1390,  ..., -0.1360, -0.0132,  0.1549]])),\n",
              "             ('lstm.weight_hh_l0',\n",
              "              tensor([[-0.7869,  0.1276, -0.3963,  ..., -0.3837,  0.0122, -0.2020],\n",
              "                      [ 0.4870, -0.0790,  0.4427,  ...,  0.2344,  0.0968, -0.2793],\n",
              "                      [ 0.5122,  0.1933, -0.7993,  ...,  0.0521, -0.0453,  0.0804],\n",
              "                      ...,\n",
              "                      [-0.1827,  0.4072,  0.3444,  ...,  0.3211, -0.0584,  0.2640],\n",
              "                      [-0.4721, -0.0240,  0.3197,  ..., -0.0027, -0.1218,  0.1307],\n",
              "                      [-0.3551, -0.2777, -0.2641,  ..., -0.0984, -0.3427,  0.4342]])),\n",
              "             ('lstm.bias_ih_l0',\n",
              "              tensor([-0.0205, -0.0860,  0.1209, -0.0322, -0.2768,  0.1386, -0.0759,  0.1308,\n",
              "                       0.3649,  0.4771,  0.2480,  0.5450,  0.1758,  0.3367,  0.1513,  0.0757,\n",
              "                      -0.0635,  0.2579,  0.6298,  0.1046,  0.1261,  0.3425,  0.0362, -0.1126,\n",
              "                      -0.7027, -0.4183, -0.2259,  0.1204, -0.3472, -0.3511, -0.4619, -0.4040,\n",
              "                       0.1234, -0.5742, -0.0129, -0.0704,  0.1431,  0.0876, -0.3861, -0.1032,\n",
              "                      -0.5162, -0.4469, -0.2267, -0.0017, -0.4582, -0.1649, -0.7227, -0.1921,\n",
              "                      -0.0566,  0.1526, -0.1538,  0.1819,  0.0642,  0.1129, -0.4071,  0.1580,\n",
              "                      -0.2184,  0.5278,  0.3513,  0.2135,  0.4536, -0.4399,  0.6135,  0.2387,\n",
              "                       0.1730, -0.2620, -0.4121, -0.5559,  0.0661, -0.4518, -0.6210, -0.0524,\n",
              "                       0.1187, -0.1301,  0.1685,  0.2295,  0.3969,  0.2921, -0.0025,  0.0998,\n",
              "                       0.4249,  0.4635,  0.4299, -0.1223,  0.7851,  1.0233,  0.5572, -0.2602,\n",
              "                       0.2614, -0.0622,  0.4850,  0.4368,  0.3608, -0.0262, -0.0081, -0.0685])),\n",
              "             ('lstm.bias_hh_l0',\n",
              "              tensor([-0.1093,  0.1459,  0.2918,  0.0171,  0.0217,  0.0472,  0.0059,  0.0716,\n",
              "                       0.1349,  0.1609,  0.1620,  0.5383,  0.3315,  0.3201,  0.1654, -0.0632,\n",
              "                      -0.2722,  0.0856,  0.2874, -0.0293,  0.0375,  0.2377,  0.1030, -0.1705,\n",
              "                      -0.4599, -0.5017, -0.2759, -0.0537, -0.1321, -0.2296, -0.7244, -0.2483,\n",
              "                      -0.0148, -0.2623,  0.1517, -0.1195,  0.2184,  0.3498, -0.4457, -0.0814,\n",
              "                      -0.1579, -0.4236,  0.0383,  0.3193, -0.4066, -0.3079, -0.5613, -0.1468,\n",
              "                       0.2058,  0.3523, -0.2682, -0.0304,  0.0753,  0.2591, -0.2858,  0.2772,\n",
              "                      -0.1933,  0.2808,  0.1985,  0.1173,  0.5388, -0.4522,  0.6238,  0.1384,\n",
              "                       0.0876, -0.1523, -0.0838, -0.4133,  0.1383, -0.4024, -0.4023,  0.0782,\n",
              "                       0.3246, -0.4201,  0.1261, -0.1034,  0.2411,  0.1799, -0.0171,  0.1257,\n",
              "                       0.3624,  0.2909,  0.2116,  0.1170,  0.7610,  0.8909,  0.5908, -0.3315,\n",
              "                       0.1923,  0.2512,  0.2980,  0.5962,  0.4669,  0.1471, -0.0222,  0.0170])),\n",
              "             ('linear.weight',\n",
              "              tensor([[ 2.5127, -2.4110,  1.5943,  2.6504, -2.1132,  1.4868,  1.5812,  2.4420,\n",
              "                        1.2512, -1.7252, -1.6932, -1.0944, -2.0199,  1.9129, -1.9836, -2.1665,\n",
              "                       -1.8350,  2.4238, -2.0187,  1.7837, -1.1109,  1.8843,  1.9031, -2.2325],\n",
              "                      [ 2.0089, -2.1130,  0.7829,  2.5013, -2.1307,  2.1390,  1.7963,  2.4319,\n",
              "                        1.3932, -2.1415, -1.9850, -1.9947, -1.5341,  1.6810, -2.0361, -2.7513,\n",
              "                       -2.1105,  2.6198, -2.1871,  1.7793, -0.5060,  1.6310,  1.3347, -2.2419],\n",
              "                      [ 1.8907, -2.2224, -0.6967,  2.2137, -1.8778,  2.4759,  2.0711,  2.3990,\n",
              "                        2.0306, -2.3210, -2.0282, -2.4080, -1.9952,  1.8441, -1.8611, -2.5398,\n",
              "                       -2.1037,  2.3665, -2.1632,  1.7457,  0.0383,  1.8906,  1.0531, -2.1209],\n",
              "                      [ 1.6903, -2.1754, -0.5981,  2.2826, -1.8169,  2.3842,  2.2034,  2.1523,\n",
              "                        2.0285, -2.1899, -1.8839, -2.2566, -1.7939,  2.1506, -1.8223, -2.4711,\n",
              "                       -2.2268,  2.3034, -2.0644,  1.9790, -0.0853,  2.1147,  1.1314, -2.0675],\n",
              "                      [ 1.8073, -2.4389, -0.6740,  2.1405, -2.0850,  2.4740,  2.2222,  2.0568,\n",
              "                        2.0070, -2.4379, -1.9671, -2.4365, -2.3696,  2.2858, -2.0737, -1.9233,\n",
              "                       -2.2739,  2.1812, -2.3313,  2.0546, -0.0300,  2.1048,  1.3462, -2.3119]])),\n",
              "             ('linear.bias',\n",
              "              tensor([-1.5667, -1.5197, -1.3495, -1.7289, -1.7879]))])"
            ]
          },
          "execution_count": 215,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.dqn.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {},
      "outputs": [],
      "source": [
        "name = 'base_Chicago_single_layer'\n",
        "torch.save(agent.dqn, f'models/{name}.pth')\n",
        "np.save(f'rewards/averaged/{name}.npy', np.array(all_rewards))\n",
        "np.save(f'rewards/episodic/{name}.npy', np.array(all_rewards_each_step))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test\n",
        "Use the held out data to test model performance and assess total reward. The closer the reward is to 0, the better. Given the architecture, the reward cannot be positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QNetwork(\n",
              "  (lstm): LSTM(82, 24, batch_first=True)\n",
              "  (linear): Linear(in_features=24, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 223,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reload model\n",
        "loaded_model = torch.load('models/base_Chicago_single_layer.pth')\n",
        "# set to evaluation mode\n",
        "loaded_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute Reward on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_X' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[224], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# init test env\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m env_test \u001b[38;5;241m=\u001b[39m TimeSeries(\u001b[43mtest_X\u001b[49m, test_y, window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m      3\u001b[0m state \u001b[38;5;241m=\u001b[39m env_test\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      4\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'test_X' is not defined"
          ]
        }
      ],
      "source": [
        "# init test env\n",
        "env_test = TimeSeries(test_X, test_y, window_size = 7)\n",
        "state = env_test.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "# compute total reward\n",
        "while not done:\n",
        "    action = loaded_model.select_action(np.squeeze(state))\n",
        "    next_state, reward, done = env_test.step(action)\n",
        "    total_reward += reward\n",
        "    state = next_state\n",
        "print(f\"Total reward on new data: {total_reward}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
